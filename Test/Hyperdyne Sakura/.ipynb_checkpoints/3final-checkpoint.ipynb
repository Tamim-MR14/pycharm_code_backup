{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71052.50000000001, 9.399999999999999, 323.09999999999997, 2.9999999999999885, 2172], [70893.39999999997, 4.5, 397.9999999999999, 41.3, 2072], [70708.49999999999, 2.9, 288.0, -65.79999999999997, 1778], [71292.59999999999, 14.900000000000002, 353.69999999999993, 95.79999999999998, 2831], [71057.19999999997, 7.5, 321.8000000000001, 33.59999999999999, 2470], [71260.0, 7.999999999999998, 360.5000000000001, 70.60000000000001, 2563], [70981.6, 11.5, 391.5999999999999, 65.39999999999999, 1945], [71158.09999999999, 16.0, 392.2999999999999, 117.09999999999998, 2790], [71060.5, 12.0, 353.6000000000001, 59.900000000000006, 2278], [71090.5, 25.0, 419.49999999999994, 179.90000000000003, 2843], [71122.50000000003, 18.5, 454.0000000000001, 186.90000000000003, 2424], [71076.1, 10.5, 364.90000000000003, 108.49999999999999, 2362], [70990.20000000004, 12.5, 367.20000000000005, 94.1, 2285], [71079.4, 18.5, 449.09999999999997, 153.29999999999998, 2487], [71140.10000000002, 8.0, 328.49999999999994, 48.599999999999994, 2168], [70928.20000000003, 17.0, 384.6, 119.59999999999998, 2304], [70993.9, 17.0, 527.3000000000001, 258.20000000000005, 2528], [71008.79999999997, 19.5, 399.69999999999993, 136.59999999999997, 2320], [71023.59999999999, 8.0, 358.1000000000001, 101.2, 2173], [71105.79999999999, 13.5, 420.50000000000006, 165.49999999999997, 2322], [71186.50000000001, 6.0, 449.1999999999999, 189.20000000000002, 2302], [71046.89999999997, 10.5, 256.20000000000005, 16.2, 2284], [71058.40000000001, 6.5, 340.0000000000001, 89.2, 2272], [71054.70000000001, 14.0, 452.5, 169.5999999999999, 2492], [71155.79999999999, 6.5, 447.19999999999993, 179.20000000000005, 2281], [71313.29999999999, 29.5, 554.6000000000001, 320.50000000000006, 2820], [71393.5, 21.5, 476.29999999999995, 266.59999999999997, 2873], [71014.5, 21.5, 473.2999999999999, 218.29999999999998, 2123], [71116.0, 12.5, 498.9, 244.60000000000002, 2531], [71093.80000000002, 21.5, 496.9999999999999, 253.4, 2506], [71029.8, 12.5, 456.4000000000001, 199.00000000000003, 2171], [71057.90000000001, 13.5, 428.1, 168.5, 2222], [71006.40000000001, 6.5, 526.2999999999998, 244.29999999999995, 1948], [71229.60000000003, 18.5, 448.1, 195.49999999999997, 2694], [70992.4, 8.5, 494.1, 206.99999999999997, 1980], [71090.59999999999, 11.5, 492.3000000000001, 241.79999999999993, 1935], [71041.1, 13.0, 420.0, 168.30000000000007, 2085], [71049.3, 22.0, 567.0000000000003, 301.79999999999995, 2105], [71088.8, 28.0, 423.19999999999993, 188.39999999999995, 2135], [71033.59999999996, 5.5, 514.3, 250.50000000000003, 1836], [71041.6, 13.0, 437.70000000000005, 185.29999999999993, 2177], [71283.40000000002, 16.5, 441.09999999999997, 214.69999999999993, 2348], [71228.29999999999, 19.5, 595.7, 355.5999999999999, 2076], [71159.5, 21.5, 517.7, 292.4, 2314], [71126.80000000002, 18.5, 482.9999999999999, 224.1, 2586], [71062.30000000003, 13.0, 422.7, 165.69999999999993, 1956], [71159.40000000002, 18.0, 386.7, 171.70000000000002, 2410], [71156.70000000001, 11.0, 468.59999999999997, 199.79999999999998, 2006], [71276.80000000002, 17.0, 430.19999999999993, 191.30000000000004, 2213], [71049.90000000001, 23.0, 424.49999999999994, 157.4, 2446], [71257.7, 16.0, 497.80000000000007, 207.99999999999994, 2515], [70993.09999999999, 7.0, 461.20000000000005, 161.70000000000002, 2275]]\n",
      "[[71045.19999999998, 17.9, 450.7, 140.59999999999997, 2490], [71059.3, 10.5, 380.19999999999993, 124.4, 2324], [71112.89999999998, 12.5, 387.7, 153.40000000000003, 2731], [70969.20000000001, 16.5, 434.7999999999999, 202.00000000000006, 2418], [71155.5, 11.5, 426.90000000000003, 182.7, 1855]]\n",
      "52\n",
      "5\n",
      "5\n",
      "[91, 91, 91, 93, 92, 89, 89, 96, 97, 88, 90, 92, 88, 82, 81, 90, 82, 91, 85, 82, 90, 102, 93, 82, 93, 79, 79, 89, 84, 83, 90, 91, 80, 86, 83, 90, 82, 75, 86, 78, 90, 80, 79, 80, 81, 87, 91, 75, 84, 82, 81, 80]\n",
      "[79, 89, 93, 90, 82]\n",
      "52\n",
      "5\n",
      "Test score 0.880155836338\n",
      "Train_score 0.526385011025\n",
      "[77, 91, 93, 88, 83]\n",
      "0.905247813411\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "#training=[]\n",
    "#test=[]\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "df=pd.read_csv(\"modified tokyo.csv\")\n",
    "k=0\n",
    "m=0\n",
    "for i in range(1961,2018):\n",
    "    #bdi=df.loc[(df['year'] == i) & df['bloom'].isin([1])].index.tolist()\n",
    "    #df2=df.loc[(df['year'] == i) & (df['serial']<=bdi[0])]\n",
    "    #df2.index=range(len(df2))\n",
    "    df2=df.loc[df['year'].isin([i])]\n",
    "    df2.index=range(len(df2))\n",
    "    df2=df2.iloc[:70,:]\n",
    "    if i in [1966,1971,1985,1994,2008]:\n",
    "        continue\n",
    "        \n",
    "    #training.append([])\n",
    "    X_train.append([])\n",
    "    #for j in range(6,12):\n",
    "        #training[m].append(df2.iloc[:,j].mean())\n",
    "        #X_train[m].append(df2.iloc[:,j].mean())\n",
    "    for j in [5,8,9,11,13]:\n",
    "        #training[m].append(df2.iloc[:,j].sum())\n",
    "        X_train[m].append(df2.iloc[:,j].sum())\n",
    "    m+=1\n",
    "\n",
    "for i in [1966,1971,1985,1994,2008]:\n",
    "    #df2=df.loc[(df['year'] == i)]\n",
    "    #df2.index=range(len(df2))\n",
    "    #df2=df2.iloc[0:60,:]\n",
    "    df2=df.loc[df['year'].isin([i])]\n",
    "    df2.index=range(len(df2))\n",
    "    df2=df2.iloc[:70,:]\n",
    "    \n",
    "    #test.append([])\n",
    "    X_test.append([])\n",
    "    #for j in range(6,12):\n",
    "        #test[k].append(df2.iloc[:,j].mean())\n",
    "        #X_test[k].append(df2.iloc[:,j].mean())\n",
    "    for j in [5,8,9,11,13]:\n",
    "        #test[k].append(df2.iloc[:,j].sum())\n",
    "        X_test[k].append(df2.iloc[:,j].sum())\n",
    "    k+=1\n",
    "\n",
    "k=0\n",
    "m=0\n",
    "\n",
    "for i in range(1961,2018):\n",
    "    bdi=df.loc[(df['year'] == i) & df['bloom'].isin([1])].index.tolist()\n",
    "    df2=df.loc[(df['year'] == i) & (df['serial']<=bdi[0])]\n",
    "    df2.index=range(len(df2))\n",
    "    \n",
    "    if i in [1966,1971,1985,1994,2008]:\n",
    "        bdi=df2.loc[(df2['year'] == i) & df2['bloom'].isin([1])].index.tolist()\n",
    "        #test[k].append(bdi[0]+1)\n",
    "        Y_test.append(bdi[0]+1)\n",
    "        k+=1\n",
    "        continue\n",
    "    \n",
    "    bdi=df2.loc[(df2['year'] == i) & df2['bloom'].isin([1])].index.tolist()\n",
    "    #training[m].append(bdi[0]+1)\n",
    "    Y_train.append(bdi[0]+1)\n",
    "    m+=1\n",
    "    \n",
    "    \n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(len(X_train))\n",
    "print(len(X_train[0]))\n",
    "print(len(X_test))\n",
    "#X_train=np.array(training)[:,:1]\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "#print(X_train)\n",
    "\n",
    "#Y_train=np.array(training)[:,-1:].ravel()\n",
    "print(Y_train)\n",
    "print(Y_test)\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))\n",
    "#print(np.mean(Y_train))\n",
    "#X_test=np.array(test)[:,:1]\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "#print(X_test)\n",
    "\n",
    "#Y_test=np.array(test)[:,-1:].ravel()\n",
    "#print(Y_test)\n",
    "\n",
    "\n",
    "#clf = MLPRegressor(max_iter = 200, solver='lbfgs', hidden_layer_sizes = (50,50), activation='identity')\n",
    "#sfs = SFS(clf,k_features=5,forward=True,floating=False,scoring='r2',cv=10)\n",
    "#sfs = sfs.fit(np.array(X_train_scaled),np.array(Y_train))\n",
    "#print('best feature columns:', sfs.k_feature_idx_)\n",
    "#X_train_sfs = sfs.transform(np.array(X_train_scaled))\n",
    "#X_test_sfs = sfs.transform(np.array(X_test_scaled))\n",
    "\n",
    "#print(X_train_sfs)\n",
    "#print(X_test_sfs)\n",
    "\n",
    "out=0\n",
    "train_out=0\n",
    "Y_pred=[0,0,0,0,0]\n",
    "for i in range(50):\n",
    "    clf=MLPRegressor(hidden_layer_sizes=(50,50),activation='identity', solver='lbfgs',max_iter=200)\n",
    "    #clf=clf.fit(X_train,Y_train)\n",
    "    clf.fit(X_train_scaled, Y_train)\n",
    "    Y=clf.predict(X_test_scaled)\n",
    "    Y_pred=[x + y for x, y in zip(Y_pred, Y)]\n",
    "    out=out+clf.score(X_test_scaled,Y_test)\n",
    "    train_out=train_out+clf.score(X_train_scaled,Y_train)\n",
    "    \n",
    "print('Test score',out/50)\n",
    "print('Train_score',train_out/50)\n",
    "Y_pred=[int(round(i/50)) for i in Y_pred]\n",
    "print(Y_pred)\n",
    "print(metrics.r2_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
